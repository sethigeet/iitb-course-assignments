\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}

\title{\textbf{Continuous \& Discrete Denoising Diffusion Probabilistic Models (D3PM \& DDPM) for MNIST Generation}}
\author{Avdhoot Golekar (23B0060), Geet Sethi (23B2258), Panav Shah (23B3323)}

\begin{document}

\maketitle

\section{Discrete Denoising Diffusion Probabilistic Model (D3PM)}

\subsection{Model Architecture}

\subsubsection{UNet Backbone}
The core architecture is based on a U-Net with residual blocks and time embeddings:

\begin{itemize}
    \item \textbf{Input/Output}: Single channel $28\times28$ images with 10 discrete classes (0-8 for pixel values, 9 for absorbing state)
    \item \textbf{Time Embedding}: Sinusoidal position embeddings of dimension 128
    \item \textbf{Downsampling Path}: Three residual blocks ($64\rightarrow128\rightarrow256$ channels) with $2\times2$ max pooling
    \item \textbf{Bottleneck}: $256\rightarrow512$ channel residual block
    \item \textbf{Upsampling Path}: Three residual blocks with skip connections and transposed convolutions
    \item \textbf{Output Layer}: $1\times1$ convolution producing 10-channel logits
\end{itemize}

\subsubsection{Residual Block Design}
Each residual block incorporates:
\begin{itemize}
    \item Two $3\times3$ convolutions with group normalization
    \item Time embedding injection via learned linear transformation
    \item ReLU activation and dropout (0.1)
    \item Skip connection with $1\times1$ convolution for dimension matching
\end{itemize}

\subsubsection{Conditional Architecture}
The conditional variant extends the UNet with:
\begin{itemize}
    \item Class embedding layer: 10 classes $\rightarrow$ 128 dimensions
    \item Class embedding addition to time embeddings
    \item Forward pass includes class labels as additional conditioning
\end{itemize}

\subsection{Training Procedure}

\subsubsection{Data Preprocessing}
MNIST images are preprocessed as follows:
\begin{enumerate}
    \item Normalize to $[0, 1]$ range using \texttt{transforms.ToTensor()}
    \item Discretize to 9 discrete values: $x_{discrete} = \text{round}(x \times 8)$
    \item Values 0-8 represent pixel intensities, 9 is the absorbing state
\end{enumerate}

\subsubsection{Loss Function}
The model uses cross-entropy loss between predicted and true discrete pixel values:
\begin{equation}
\mathcal{L} = \text{CrossEntropy}(\text{logits}_{flat}, \text{targets}_{flat})
\end{equation}

\subsubsection{Noise Scheduling}

We use the following noise scheduling methods to choose the mask probabilities for each of the input tokens:

\subsubsection{Linear Schedule}
\begin{equation}
\text{mask\_prob}(t) = \frac{t}{T-1}
\end{equation}
where $T$ is the total number of timesteps.

\subsubsection{Cosine Schedule}
\begin{equation}
\alpha_t = \cos\left(\frac{\pi}{2} \cdot \frac{t + s}{T + s}\right)^2
\end{equation}
where $s = 0.008$ for numerical stability.

\subsubsection{Training Algorithm}

\begin{algorithm}[H]
\caption{D3PM Training Loop}
\begin{algorithmic}[1]
\STATE Initialize model $\theta$, optimizer, scheduler
\FOR{epoch = 1 to max\_epochs}
    \FOR{batch in dataloader}
        \STATE Sample timesteps $t \sim \text{Uniform}(0, T)$
        \STATE Add noise: $x_t = \text{scheduler.add\_noise}(x_0, t, \text{num\_classes})$
        \STATE Forward pass: $\hat{x}_0 = \text{model}(x_t, t, \text{class\_label})$
        \STATE Compute loss: $\mathcal{L} = \text{CrossEntropy}(\hat{x}_0, x_0)$
        \STATE Backward pass and update $\theta$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Sampling Procedure}

\begin{algorithm}[H]
\caption{D3PM Sampling}
\begin{algorithmic}[1]
\STATE Initialize $x_T$ with absorbing states (value 9)
\FOR{$t = T-1$ down to 0}
    \STATE Get logits: $\text{logits} = \text{model}(x_t, t, \text{class\_label})$
    \STATE Convert to probabilities: $p = \text{softmax}(\text{logits})$
    \STATE Sample: $x_{t-1} \sim \text{Categorical}(p)$
\ENDFOR
\STATE Convert to continuous: $x_{final} = x_0 / 8$
\end{algorithmic}
\end{algorithm}

\subsection{Experimental Setup}

\subsubsection{Hyperparameters}
All experiments use the following base configuration:
\begin{itemize}
    \item \textbf{Epochs}: 300
    \item \textbf{Batch Size}: 64
    \item \textbf{Learning Rate}: 0.0001
    \item \textbf{Optimizer}: Adam
    \item \textbf{Evaluation Samples}: 1000
    \item \textbf{FID Batches}: 5 (64 samples each)
\end{itemize}

\subsubsection{Experimental Variables}
We vary the following parameters:
\begin{itemize}
    \item \textbf{Diffusion Steps}: 100, 500, 1000
    \item \textbf{Scheduler Type}: Linear, Cosine
    \item \textbf{Model Type}: Unconditional D3PM, Conditional D3PM
\end{itemize}

\subsection{Results and Analysis}

\subsubsection{FID Score Results}
Table \ref{tab:results-d3pm} presents the FID scores for all experimental configurations:

\begin{table}[H]
\centering
\caption{FID Scores Across All Experimental Configurations}
\label{tab:results-d3pm}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model Type} & \textbf{Scheduler} & \textbf{Steps} & \textbf{FID Score} \\
\midrule
\multirow{3}{*}{D3PM (Unconditional)} & Cosine & 1000 & 198.02 \\
& Cosine & 500 & 183.44 \\
& Cosine & 100 & 195.70 \\
& Linear & 1000 & 180.53 \\
\midrule
\multirow{3}{*}{D3PM (Conditional)} & Cosine & 1000 & 133.13 \\
& Cosine & 500 & 149.55 \\
& Cosine & 100 & 175.50 \\
& Linear & 1000 & 226.67 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Key Observations}

\begin{itemize}
    \item Conditional models consistently outperform unconditional models across all configurations
    \item Cosine scheduler is generally superior for both model types but more prominently seen in conditional models
    \item More diffusion steps generally improve quality for both model types however the improvement is not as significant as the number of steps increases
\end{itemize}

\subsubsection{Sample Visualizations}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_300ep_64bs_0.0001lr_100steps_cosinescheduler_model.pth.png}
        \caption{T=100}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_300ep_64bs_0.0001lr_500steps_cosinescheduler_model.pth.png}
        \caption{T=500}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{T=1000}
    \end{subfigure}
    \caption{Final outputs of the unconditional D3PM with cosine schedule for different diffusion steps}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_300ep_64bs_0.0001lr_1000steps_linearscheduler_model.pth.png}
        \caption{Linear schedule}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{Cosine schedule}
    \end{subfigure}
    \caption{Final outputs of the unconditional D3PM with different schedules for 1000 steps}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_cond_300ep_64bs_0.0001lr_100steps_cosinescheduler_model.pth.png}
        \caption{T=100}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_cond_300ep_64bs_0.0001lr_500steps_cosinescheduler_model.pth.png}
        \caption{T=500}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_cond_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{T=1000}
    \end{subfigure}
    \caption{Final outputs of the conditional D3PM with cosine schedule for different diffusion steps.}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_cond_300ep_64bs_0.0001lr_1000steps_linearscheduler_model.pth.png}
        \caption{Linear schedule}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_d3pm_cond_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{Cosine schedule}
    \end{subfigure}
    \caption{Final outputs of the conditional D3PM with different schedules for 1000 steps}
\end{figure}

% ------------------------------------------------------------------------

\section{Continuous Denoising Diffusion Probabilistic Model (DDPM)}

\subsection{Model Architecture}

\subsubsection{UNet Backbone}
We use the same UNet backbone as the D3PM model described in the previous section.

\subsection{Training Procedure}

\subsubsection{Data Preprocessing}
MNIST images are preprocessed as follows:
\begin{enumerate}
    \item Normalize to [0, 1] range using \texttt{transforms.ToTensor()}
    \item Scale to model range [-1, 1]: $x_{model} = 2x - 1$
    \item Maintain continuous values throughout training
\end{enumerate}

\subsubsection{Loss Function}
The model uses Mean Squared Error (MSE) loss between predicted and actual noise:
\begin{equation}
\mathcal{L} = \text{MSE}(\epsilon_{\theta}(x_t, t), \epsilon)
\end{equation}
where $\epsilon$ is the actual noise added and $\epsilon_{\theta}(x_t, t)$ is the model's prediction.

\subsubsection{Noise Scheduling}

We use the following noise scheduling methods to add noise to the input images during training as well as sampling:

\subsubsection{Linear Schedule}
\begin{align}
\beta_t &= \beta_1 + \frac{t}{T-1}(\beta_T - \beta_1) \\
\alpha_t &= 1 - \beta_t \\
\bar{\alpha}_t &= \prod_{s=1}^t \alpha_s
\end{align}
where $\beta_1 = 10^{-4}$ and $\beta_T = 2 \times 10^{-2}$.

\subsubsection{Cosine Schedule}
\begin{align}
\bar{\alpha}_t &= \frac{f(t)}{f(0)} \\
f(t) &= \cos\left(\frac{\pi}{2} \cdot \frac{t + s}{T + s}\right)^2
\end{align}
where $s = 0.008$ for numerical stability.

\subsubsection{Forward Diffusion Process}
The forward process gradually adds Gaussian noise:
\begin{equation}
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t \mathbf{I})
\end{equation}

The reparameterization trick allows sampling $x_t$ directly from $x_0$:
\begin{equation}
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon
\end{equation}

\subsubsection{Training Algorithm}
\begin{algorithm}[H]
\caption{DDPM Training Loop}
\begin{algorithmic}[1]
\STATE Initialize model $\theta$, optimizer, scheduler
\FOR{epoch = 1 to max\_epochs}
    \FOR{batch in dataloader}
        \STATE Scale data to $[-1, 1]$ range: $x_0 = 2x - 1$
        \STATE Sample timesteps $t \sim \text{Uniform}(0, T)$
        \STATE Sample noise: $\epsilon \sim \mathcal{N}(0, \mathbf{I})$
        \STATE Add noise: $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$
        \STATE Forward pass: $\hat{\epsilon} = \text{model}(x_t, t)$
        \STATE Compute loss: $\mathcal{L} = \text{MSE}(\hat{\epsilon}, \epsilon)$
        \STATE Backward pass and update $\theta$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Sampling Procedure}

\subsubsection{Reverse Diffusion Process}
Sampling follows the reverse process using the learned model:

\begin{algorithm}[H]
\caption{DDPM Sampling}
\begin{algorithmic}[1]
\STATE Initialize $x_T \sim \mathcal{N}(0, \mathbf{I})$
\FOR{$t = T-1$ down to 0}
    \STATE Predict noise: $\hat{\epsilon} = \text{model}(x_t, t)$
    \STATE Compute predicted mean: $\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\hat{\epsilon}\right)$
    \STATE Sample next $x$: $x_{t-1} \sim \mathcal{N}(\mu_\theta(x_t, t), \sigma_t^2 \mathbf{I})$
\ENDFOR
\STATE Scale back to image range ($[0, 1]$): $x_0 = \text{clamp}(\frac{x_0 + 1}{2}, 0, 1)$
\end{algorithmic}
\end{algorithm}

\subsubsection{Conditional Sampling}
For conditional generation, class labels are provided at each timestep:
\begin{equation}
\hat{\epsilon} = \text{model}(x_t, t, \text{class\_label})
\end{equation}

\subsubsection{Reverse Process Variance}
The reverse process variance is computed as:
\begin{equation}
\sigma_t^2 = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t
\end{equation}

\subsection{Experimental Setup}

\subsubsection{Hyperparameters}
All experiments use the following base configuration:
\begin{itemize}
    \item \textbf{Epochs}: 300
    \item \textbf{Batch Size}: 64
    \item \textbf{Learning Rate}: 0.0001
    \item \textbf{Optimizer}: Adam
    \item \textbf{Evaluation Samples}: 1000
    \item \textbf{FID Batches}: 5 (64 samples each)
\end{itemize}

\subsubsection{Experimental Variables}
We vary the following parameters:
\begin{itemize}
    \item \textbf{Diffusion Steps}: 100, 500, 1000
    \item \textbf{Scheduler Type}: Linear, Cosine
    \item \textbf{Model Type}: Unconditional DDPM, Conditional DDPM
\end{itemize}

\subsection{Implementation Details}

\subsubsection{Noise Scheduler Implementation}
The \texttt{NoiseSchedulerDDPM} class handles the following:
\begin{itemize}
    \item Precomputation of $\alpha_t$, $\bar{\alpha}_t$, $\beta_t$
    \item Forward process: $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$
    \item Reverse process: $x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z$
\end{itemize}

\subsubsection{Model Architecture Details}
Same as the D3PM model described in the previous section.

\subsection{Results and Analysis}

\subsubsection{FID Score Results}
Table \ref{tab:results-ddpm} presents the FID scores for all experimental configurations:

\begin{table}[H]
\centering
\caption{FID Scores Across All Experimental Configurations}
\label{tab:results-ddpm}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model Type} & \textbf{Scheduler} & \textbf{Steps} & \textbf{FID Score} \\
\midrule
\multirow{3}{*}{DDPM (Unconditional)} & Cosine & 1000 & 52.39 \\
& Cosine & 500 & 52.86 \\
& Cosine & 100 & 53.07 \\
& Linear & 1000 & 139.99 \\
\midrule
\multirow{3}{*}{DDPM (Conditional)} & Cosine & 1000 & 54.26 \\
& Cosine & 500 & 55.01 \\
& Cosine & 100 & 55.24 \\
& Linear & 1000 & 155.29 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Key Observations}

\begin{itemize}
    \item Cosine scheduler is generally superior for both model types but more prominently seen in conditional models
    \item More diffusion steps generally improve quality for both model types however the improvement is not as significant as the number of steps increases
\end{itemize}

\subsubsection{Sample Visualizations}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_300ep_64bs_0.0001lr_100steps_cosinescheduler_model.pth.png}
        \caption{T=100}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_300ep_64bs_0.0001lr_500steps_cosinescheduler_model.pth.png}
        \caption{T=500}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{T=1000}
    \end{subfigure}
    \caption{Final outputs of the unconditional DDPM with cosine schedule for different diffusion steps}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_300ep_64bs_0.0001lr_1000steps_linearscheduler_model.pth.png}
        \caption{Linear schedule}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{Cosine schedule}
    \end{subfigure}
    \caption{Final outputs of the unconditional DDPM with different schedules for 1000 steps}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_cond_300ep_64bs_0.0001lr_100steps_cosinescheduler_model.pth.png}
        \caption{T=100}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_cond_300ep_64bs_0.0001lr_500steps_cosinescheduler_model.pth.png}
        \caption{T=500}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_cond_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{T=1000}
    \end{subfigure}
    \caption{Final outputs of the conditional DDPM with cosine schedule for different diffusion steps.}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_cond_300ep_64bs_0.0001lr_1000steps_linearscheduler_model.pth.png}
        \caption{Linear schedule}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{samples/samples_ddpm_cond_300ep_64bs_0.0001lr_1000steps_cosinescheduler_model.pth.png}
        \caption{Cosine schedule}
    \end{subfigure}
    \caption{Final outputs of the conditional DDPM with different schedules for 1000 steps}
\end{figure}

\section{References}

\begin{itemize}
    \item \href{https://huggingface.co/papers/2006.11239}{Denoising Diffusion Probabilistic Models - Hugging Face}
    \item \href{https://github.com/lucidrains/denoising-diffusion-pytorch}{Denoising Diffusion PyTorch - GitHub}
    \item \href{https://medium.com/@sjasmeet135/denoising-diffusion-model-implementation-from-scratch-b0a1fc6ef5d8}{Denoising Diffusion Model Implementation from Scratch - Medium}
    \item \href{https://github.com/Abdennacer-Badaoui/D3PMs}{D3PMs - GitHub}
    \item ChatGPT \& Google Gemini
\end{itemize}

\end{document}