{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96ec7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/extusr/sethigeet/assignment-3/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3eab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# HF_TOKEN = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74b32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "553c04ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]\n",
      "/users/extusr/sethigeet/assignment-3/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py:789: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=None)\n",
    "model.eval()\n",
    "model.to(DEVICE) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec031e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_PROMPT = \"The wind whispered through old ruins\"\n",
    "INSTRUCTION_PROMPT = \"Continue the story.\"\n",
    "PROMPT = f\"{PREFIX_PROMPT}\\n\\n{INSTRUCTION_PROMPT}.\\n\"\n",
    "MAX_NEW_TOKENS = 50\n",
    "BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7509ceb",
   "metadata": {},
   "source": [
    "## Simple Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da576da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "input_ids = input_ids.repeat(BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb3c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_ids = torch.zeros(BATCH_SIZE).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadba8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aaa8315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 128256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a00d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output.logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a634639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_logits, top_k_indices = torch.topk(logits, k=3, dim=-1)\n",
    "probs = torch.softmax(top_k_logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aee903e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "921b3347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2170, 791, 791]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_ids = []\n",
    "for i in range(BATCH_SIZE):\n",
    "    if completed_ids[i]:\n",
    "        next_token_ids.append(model.config.eos_token_id)\n",
    "        continue\n",
    "\n",
    "    sampled_idx = torch.multinomial(probs[i], 1)\n",
    "    next_token_id = top_k_indices[i, sampled_idx].item()\n",
    "    next_token_ids.append(next_token_id)\n",
    "\n",
    "next_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f920d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(\n",
    "    [\n",
    "        input_ids,\n",
    "        torch.tensor(next_token_ids).to(model.device).unsqueeze(-1),\n",
    "    ],\n",
    "    dim=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06b627d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,    791,  10160,  58366,   1555,   2362,  46762,    271,  24433,\n",
       "            279,   3446,  35047,   2170],\n",
       "        [128000,    791,  10160,  58366,   1555,   2362,  46762,    271,  24433,\n",
       "            279,   3446,  35047,    791],\n",
       "        [128000,    791,  10160,  58366,   1555,   2362,  46762,    271,  24433,\n",
       "            279,   3446,  35047,    791]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(MAX_NEW_TOKENS):\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[~completed_ids, -1, :]\n",
    "\n",
    "    # Get top-k logits and indices and convert to probabilities\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "    probs = torch.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "    # Sample from the top-k distribution\n",
    "    for i in range(len(logits)):\n",
    "        sampled_idx = torch.multinomial(probs[i], 1)\n",
    "        next_token_id = top_k_indices[i, sampled_idx]\n",
    "\n",
    "        # Stop if EOS token is reached\n",
    "        if next_token_id == model.config.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids for next iteration\n",
    "        input_ids[i] = torch.cat(\n",
    "            [input_ids[i], torch.tensor([[next_token_id]]).to(model.device)], dim=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8eab56",
   "metadata": {},
   "source": [
    "## Reward Function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cca93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_jsonl\n",
    "\n",
    "data = load_jsonl(\"data/outputs_task1_IS.jsonl\")\n",
    "\n",
    "samples = [[s[\"text\"] for s in d[\"continuations\"][0][\"samples\"]] for d in data]\n",
    "samples = np.array(samples).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8458f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. It carried a faint whisper. The villagers believed it was a sign of the past. But I knew better. It was a sign of the future. The wind brought with it a chill. It was as if the very presence of it was a'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = samples[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07707a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_file = \"tinystories_ngrams/trigram_probs.pkl\"\n",
    "\n",
    "with open(cache_file, \"rb\") as f:\n",
    "    cache = pickle.load(f)\n",
    "tri_probs = cache[\"trigram_probs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8feb566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'It', 'carried', 'a', 'faint', 'whisper.', 'The', 'villagers', 'believed', 'it', 'was', 'a', 'sign', 'of', 'the', 'past.', 'But', 'I', 'knew', 'better.', 'It', 'was', 'a', 'sign', 'of', 'the', 'future.', 'The', 'wind', 'brought', 'with', 'it', 'a', 'chill.', 'It', 'was', 'as', 'if', 'the', 'very', 'presence', 'of', 'it', 'was', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.18322252988756"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "class _TokenLM:\n",
    "    \"\"\"Minimal token-trigram LM with logp only. Internal use.\"\"\"\n",
    "\n",
    "    def __init__(self, tri_probs: Dict[str, float], eps: float):\n",
    "        self._tri = tri_probs\n",
    "        self._eps = eps\n",
    "\n",
    "    @staticmethod\n",
    "    def _key(t1: str, t2: str, t3: str) -> str:\n",
    "        return f\"Ġ{t1},Ġ{t2},Ġ{t3}\"\n",
    "\n",
    "    def logp(self, t1: str, t2: str, t3: str) -> float:\n",
    "        \"\"\"Return log P(t3 | t1, t2) with epsilon floor.\"\"\"\n",
    "        p = self._tri.get(self._key(t1, t2, t3), 0.0)\n",
    "        if p <= 0.0:\n",
    "            return 0.0\n",
    "        return -math.log(p)\n",
    "\n",
    "token_lm = _TokenLM(tri_probs, 1e-9)\n",
    "\n",
    "tokens = sample.split(\" \")\n",
    "print(tokens)\n",
    "\n",
    "reward = 0.0\n",
    "for i in range(len(tokens) - 2):\n",
    "    reward += token_lm.logp(tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796beb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
